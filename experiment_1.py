# -*- coding: utf-8 -*-
"""experiment_1.ipynb

Automatically generated by Colab.

"""

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertModel
import numpy as np
import pandas as pd
from scipy import stats

from transformers import DistilBertModel, DistilBertTokenizer

class StudentBertRegressor(nn.Module):
    def __init__(self, bert_model_name='distilbert-base-uncased', dropout=0.1):
        super(StudentBertRegressor, self).__init__()
        self.bert = DistilBertModel.from_pretrained(bert_model_name)
        hidden_size = self.bert.config.hidden_size

        # Head for toxicity prediction
        self.prediction_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

        # Head for distillation (producing a "hint" representation)
        self.distill_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )

    def forward(self, query_input_ids, query_attention_mask, output_attentions=False):
        outputs = self.bert(
            input_ids=query_input_ids,
            attention_mask=query_attention_mask,
            output_attentions=output_attentions
        )
        # Use the first token representation as the sentence embedding
        query_rep = outputs.last_hidden_state[:, 0]  # shape: [batch, hidden_size]
        pred = self.prediction_head(query_rep)       # toxicity prediction
        hint = self.distill_head(query_rep)            # intermediate representation for distillation
        if output_attentions:
            # Return attentions along with prediction and hint.
            # outputs.attentions is a tuple containing attention maps from all layers.
            return pred.squeeze(), hint, outputs.attentions
        else:
            return pred.squeeze(), hint

import pandas as pd
df = pd.read_csv("hf://datasets/ppaudel/loki-keyword-suggestions/raw_data_keywords.csv")
random_samples=df.sample(100)

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def inference_student_model(student_model, data_loader, device):
    """
    Evaluate the student model (query-only) and return predictions

    Assumes each batch in data_loader contains:
      - 'query_input_ids'
      - 'query_attention_mask'
      - 'label'

    Returns:
      - predictions: np.array of predicted toxicity scores
      - actual_values: np.array of ground truth labels
      - metrics: a dictionary containing mse, rmse, mae, and r2 scores.
    """
    student_model.eval()
    predictions = []
    #actual_values = []

    with torch.no_grad():
        for batch in data_loader:
            query_input_ids = batch['query_input_ids'].to(device)
            query_attention_mask = batch['query_attention_mask'].to(device)
            #labels = batch['label'].to(device)

            # Use mixed precision for faster inference
            with torch.cuda.amp.autocast():
                # Student model returns (toxicity prediction, hint), we only need the prediction.
                outputs, _ = student_model(query_input_ids, query_attention_mask)

            predictions.extend(outputs.cpu().numpy())
            #actual_values.extend(labels.cpu().numpy())

    predictions = np.array(predictions)
    #actual_values = np.array(actual_values)

    return predictions

"""### Load from HF and test"""

import torch
from huggingface_hub import hf_hub_download

# 1) Fetch the binary from your HF repo
bin_path = hf_hub_download(
    repo_id="ppaudel/loki-model-pretrained",
    filename="pytorch_model.bin"
)

# 2) Load the weights
state_dict = torch.load(bin_path, map_location="cuda")

# 3) Instantiate and populate your model
model = StudentBertRegressor()            # same signature as when you saved
model.load_state_dict(state_dict, strict=True)
model.eval()

model = model.to(device)

class PrecomputedTeacherTextRegressionDataset(Dataset):
    def __init__(self, random_files, tokenizer, query_column='query', label_column='label',
                 serp_columns=None, max_length=24):
        """
        Args:
            csv_file (str): Path to the CSV file.
            tokenizer: BERT tokenizer.
            query_column (str): Column name for the query text.
            label_column (str): Column name for the label.
            serp_columns (list or int): List of column names for SERP descriptions or an integer
                                        indicating the number of SERP columns.
            max_length (int): Maximum length for tokenization.
        """
        self.df = random_files
        self.queries = self.df[query_column].values
        #self.labels = self.df[label_column].values
        #self.serp_columns = [f'serp_{i}' for i in range(0, 11)]
        self.tokenizer = tokenizer
        self.max_length = 128

        # Precompute tokenizations for queries
        self.precomputed_query_encodings = [
            self.tokenizer(
                str(query),
                add_special_tokens=True,
                max_length=24,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            for query in self.queries
        ]

        # Precompute tokenizations for SERP descriptions
    def __len__(self):
        return len(self.queries)

    def __getitem__(self, idx):
        query_encoding = self.precomputed_query_encodings[idx]
        return {
            'query_input_ids': query_encoding['input_ids'].flatten(),
            'query_attention_mask': query_encoding['attention_mask'].flatten(),
        }

TEXT_COLUMN = 'keyword'  # Change these column names to match your CSV
BATCH_SIZE = 16

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')

print(random_samples)

test_dataset = PrecomputedTeacherTextRegressionDataset(random_samples, tokenizer, TEXT_COLUMN)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

result = inference_student_model(model,test_loader,device)

print(len(result))

# 1. Add the predictions as a new column
random_samples["predicted_toxicity"] = result

# 2. Sort by that column (highest toxicity first)
result_sorted = random_samples.sort_values("predicted_toxicity", ascending=False)

# 3. (Optional) Inspect
print(result_sorted)

