# -*- coding: utf-8 -*-
"""experiment_2.ipynb

Automatically generated by Colab.


### Table 1 : Attribute based analysis
"""

import os
import pandas as pd
import numpy as np
from huggingface_hub import hf_hub_download
from datasets import load_dataset
from datasets import load_from_disk
import sys
import json

#Specify same cache as used in load models and datasets.py to effectively utilize caching of datasets and models
HF_CACHE = "./HF_CACHE"
ZENODO_DATASET_PATH = os.path.join(HF_CACHE, "zenodo_data", "loki-test-folds")
ZENODO_MODEL_DIR = os.path.join(HF_CACHE, "zenodo_data")


def read_json_as_dict(filepath):
  try:
    with open(filepath, 'r') as file:
      data = json.load(file)
      return data
  except FileNotFoundError:
    print(f"Error: File '{filepath}' not found.")
    return None
  except json.JSONDecodeError:
    print(f"Error: Invalid JSON format in '{filepath}'.")
    return None

keywords_data = read_json_as_dict('keywords_enriched.json')
keyword_score_expansion  = read_json_as_dict("keyword_expansion.json")
top50_keywords_categories=read_json_as_dict("keywords_categories.json")

from datasets import load_dataset

#fold_data = load_dataset("ppaudel/loki-test-folds")
# Load dataset from Zenodo
print(f"▶️  Loading dataset from {ZENODO_DATASET_PATH} …")
fold_data = load_from_disk(ZENODO_DATASET_PATH)


import numpy as np
import pandas as pd

def compute_fold_scores(df_test,
                        heuristics,
                        num_experiments=1000,
                        experiment_size=20):
    """
    For one fold, compute mean-of-means toxicity and expansions for each heuristic.
    Returns a dict: {heuristic: (tox_mean, exp_mean), ...}
    """
    results = {}
    texts = df_test['text'].unique()
    for heur in heuristics:
        # gather all scores & expansions for keywords satisfying this heuristic
        scores = []
        exps   = []
        for kw in texts:
            if kw in keywords_data and keywords_data[kw].get(heur, False):
                rows = df_test[df_test['text'] == kw]
                scores.extend(rows['score'].tolist())
                exps.append(keyword_score_expansion.get(kw, []))

        if not scores or not exps:
            results[heur] = (np.nan, np.nan)
            continue

        k = min(experiment_size, len(scores))
        # Toxicity: sample k without replacement, repeat, take means
        tox_means = []
        for _ in range(num_experiments):
            samp = np.random.choice(scores, size=k, replace=False)
            tox_means.append(np.mean(samp))

        # Expansions: same procedure
        exp_means = []
        for _ in range(num_experiments):
            samp = np.random.choice(exps, size=k, replace=False)
            exp_means.append(np.mean(samp))

        results[heur] = (np.mean(tox_means),
                         np.mean(exp_means))
    return results


import pandas as pd
import numpy as np

def generate_performance_table(fold_data,
                               heuristics,
                               num_experiments=1000,
                               experiment_size=10):
    """
    """
    folds = list(fold_data.keys())
    cols = pd.MultiIndex.from_product([folds, ['Tox.', 'Exp.']],
                                      names=['Fold','Metric'])
    table = pd.DataFrame(index=heuristics, columns=cols)

    for fold in folds:
        raw = fold_data[fold]

        # 1) normalize to a pandas DataFrame
        if hasattr(raw, "to_pandas"):
            df_test = raw.to_pandas()
        elif isinstance(raw, str):
            df_test = pd.read_csv(raw)
        elif isinstance(raw, pd.DataFrame):
            df_test = raw
        else:
            raise ValueError(f"Unrecognized type for fold_data['{fold}']: {type(raw)}")

        # 2) compute your scores on that DataFrame
        res = compute_fold_scores(df_test,
                                  heuristics,
                                  num_experiments,
                                  experiment_size)

        # 3) fill the table
        for heur in heuristics:
            tox, exp = res.get(heur, (np.nan, np.nan))
            table.loc[heur, (fold, 'Tox.')] = f"{tox:.3f}"
            table.loc[heur, (fold, 'Exp.')] = f"{exp:.2f}"

    return table



heuristics = [
    'informational',
    'commercial',
    'low_competiton',
    'medium_competition',
    'longtail',
]

perf_table = generate_performance_table(fold_data, heuristics)
print(perf_table)

"""### Table 2 : NER Based"""

from collections import defaultdict
import json

def merge_bio_tokens(token_list):
    merged_tokens = []
    current = None
    current_entity = None  # Holds the entity type without the BIO prefix

    for token in token_list:
        # Extract the pure entity type (e.g., "core_product_type" from "B-core_product_type")
        if token['entity'].startswith(('B-', 'I-')):
            token_entity = token['entity'][2:]  # Remove the B- or I- prefix
        else:
            token_entity = token['entity']  # Keep as is for O tags

        if token['entity'].startswith("B-"):
            # If we have a current entity group, add it to our results before starting a new one
            if current:
                merged_tokens.append(current)
            # Start a new entity
            current = token.copy()
            current_entity = token_entity

        elif token['entity'].startswith("I-"):
            # If there's no current group or entity type doesn't match, treat as B-
            if current is None or token_entity != current_entity:
                # Add current entity if exists
                if current:
                    merged_tokens.append(current)
                # Start a new entity with B- prefix
                current = token.copy()
                current['entity'] = "B-" + token_entity
                current_entity = token_entity
            else:
                # Merge with current entity
                if 'word' in token and token['word'].startswith("##"):
                    # For BERT-style subword tokens that start with ##
                    current['word'] += token['word'][2:]
                else:
                    # Add space before appending the word
                    current['word'] += " " + token['word']

        else:  # Outside tag (O) or any non-BIO tag
            # Finish current entity if it exists
            if current:
                merged_tokens.append(current)
            # Add this token as is
            current = token.copy()
            current_entity = None  # No entity type for O tags

    # Don't forget to add the last entity
    if current:
        merged_tokens.append(current)

    return merged_tokens

ff=open("query_ner_output.json")
word_entities=defaultdict(list)
entities_word=defaultdict(list)
category_entities=defaultdict(lambda: defaultdict(list))
keyword_results={}
for f in ff:
    jsx=json.loads(f)
    for idx in range(0,len(jsx["batch"])):
        keyword=jsx["batch"][idx]
        result=jsx["results"][idx]
        processed_result=merge_bio_tokens(result)
        keyword_results[keyword]=result
        for rslt in processed_result:
            if len(rslt["word"])<2 or "#" in rslt["word"]:
                continue
            #Adding this for category wise aggregration
            for category in top50_keywords_categories[keyword]:
                #we'll get a category
                category_entities[category][rslt["entity"]].append(rslt["word"])
            entities_word[rslt["entity"]].append(rslt["word"])
            word_entities[rslt["word"]].append(rslt["entity"])

eval_entities=["B-core_product_type","B-content","B-product_name",
              "B-modifier","B-price"]

import re
import numpy as np
import pandas as pd
from collections import Counter

def compute_ner_fold_scores(
    df_test: pd.DataFrame,
    top50_categories: dict,
    category_entities: dict,
    keyword_score_expansion: dict,
    eval_entities: list,
    num_experiments: int = 1000,
    experiment_size: int = 20,
    min_count: int = 10,
    top_n: int = 5
) -> dict:
    """
    For a single fold (df_test + all supporting dicts),
    compute mean-of-means toxicity and expansions for each entity in eval_entities.
    Returns { entity: (toxicity_mean, expansion_mean), ... }.
    """
    results = {}
    for ent in eval_entities:
        matching_scores = []
        matching_exps   = []

        for _, row in df_test.iterrows():
            txt = row['text']
            if txt not in top50_categories:
                continue

            cats = set(top50_categories[txt])
            # collect all predicted entities for this text under all its categories
            all_ents = []
            for cat in cats:
                all_ents.extend(category_entities.get(cat, {}).get(ent, []))

            if not all_ents:
                continue

            # take the top‐N most frequent
            top_hits = [e for e, _ in Counter(all_ents).most_common(top_n)]

            # if any of those hits occurs as a whole‐word in the text, record a match
            low_txt = txt.lower()
            for tgt in top_hits:
                pattern = r'\b' + re.escape(tgt.lower()) + r'\b'
                if re.search(pattern, low_txt):
                    matching_scores.append(row['score'])
                    matching_exps.append(keyword_score_expansion.get(txt, 0))
                    break

        # if not enough matches, skip
        if len(matching_scores) < min_count:
            results[ent] = (np.nan, np.nan)
            continue

        # sampling experiments
        k = min(experiment_size, len(matching_scores))
        scores_arr = np.array(matching_scores)
        exps_arr   = np.array(matching_exps)

        tox_means = [
            np.mean(np.random.choice(scores_arr, size=k, replace=False))
            for _ in range(num_experiments)
        ]
        exp_means = [
            np.mean(np.random.choice(exps_arr, size=k, replace=False))
            for _ in range(num_experiments)
        ]

        results[ent] = (np.mean(tox_means), np.mean(exp_means))

    return results


def generate_ner_performance_table(
    fold_data: dict,
    eval_entities: list,
    category_entities: dict,
    top50_categories: dict,
    keyword_score_expansion: dict,
    num_experiments: int = 1000,
    experiment_size: int = 20,
    min_count: int = 10,
    top_n: int = 5,
) -> pd.DataFrame:
    """
    fold_data: dict mapping fold name → (DatasetSplit | CSV-path | pd.DataFrame)
    eval_entities: list of entity names to evaluate
    category_entities, top50_categories, keyword_score_expansion: dicts as before
    Returns a DataFrame indexed by eval_entities, with MultiIndex columns (fold, [Tox., Exp.]).
    """
    folds = list(fold_data.keys())
    cols = pd.MultiIndex.from_product([folds, ['Tox.', 'Exp.']],
                                      names=['Fold','Metric'])
    table = pd.DataFrame(index=eval_entities, columns=cols)

    for fold in folds:
        raw = fold_data[fold]

        # 1) Normalize to pandas DataFrame
        if hasattr(raw, "to_pandas"):
            df = raw.to_pandas()
        elif isinstance(raw, str):
            df = pd.read_csv(raw)
        elif isinstance(raw, pd.DataFrame):
            df = raw.copy()
        else:
            raise ValueError(f"Unrecognized type for fold_data['{fold}']: {type(raw)}")

        # 2) Compute your NER‐based fold scores
        res = compute_ner_fold_scores(
            df,
            top50_categories,
            category_entities,
            keyword_score_expansion,
            eval_entities,
            num_experiments,
            experiment_size,
            min_count,
            top_n
        )

        # 3) Populate table
        for ent in eval_entities:
            tox, exp = res.get(ent, (np.nan, np.nan))
            table.loc[ent, (fold, 'Tox.')] = f"{tox:.3f}" if not np.isnan(tox) else 'NaN'
            table.loc[ent, (fold, 'Exp.')] = f"{exp:.2f}" if not np.isnan(exp) else 'NaN'

    return table


perf_table = generate_ner_performance_table(
    fold_data,
    eval_entities,
    category_entities,
    top50_keywords_categories,
    keyword_score_expansion,
    num_experiments=1000,
    experiment_size=20,
    min_count=10,
    top_n=5
)

print(perf_table)

"""### Table 3 : Teacher Model"""

from transformers import DistilBertTokenizer, DistilBertModel
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertModel
import numpy as np
import pandas as pd
from scipy import stats

class PrecomputedTeacherTextRegressionDataset(Dataset):
    def __init__(self, df_or_path, tokenizer, query_column='query', label_column='label',
                 serp_columns=None, max_length=24):
        """
        Args:
            csv_file (str): Path to the CSV file.
            tokenizer: BERT tokenizer.
            query_column (str): Column name for the query text.
            label_column (str): Column name for the label.
            serp_columns (list or int): List of column names for SERP descriptions or an integer
                                        indicating the number of SERP columns.
            max_length (int): Maximum length for tokenization.
        """
        # load into DataFrame
        if isinstance(df_or_path, str):
            if df_or_path.endswith('.csv'):
                df = pd.read_csv(df_or_path)
            elif df_or_path.endswith('.arrow') or df_or_path.endswith('.feather'):
                df = pd.read_feather(df_or_path)
            else:
                raise ValueError(f"Unrecognized file type: {df_or_path}")
        elif hasattr(df_or_path, "to_pandas"):
            df = df_or_path.to_pandas()
        else:
            df = df_or_path

        self.df = df #pd.read_csv(dataframe)#.to_pandas()#pd.read_csv(csv_file)
        self.queries = self.df[query_column].values
        self.labels = self.df[label_column].values

        self.serp_columns = [f'serp_{i}' for i in range(0, 11)]#prev 11

        self.tokenizer = tokenizer
        self.max_length = 128#Prev 128

        # Precompute tokenizations for queries
        self.precomputed_query_encodings = [
            self.tokenizer(
                str(query),
                add_special_tokens=True,
                max_length=24,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            for query in self.queries
        ]

        # Precompute tokenizations for SERP descriptions
        self.precomputed_serp_encodings = []
        for idx in range(len(self.df)):
            serp_texts = [str(self.df.iloc[idx][col]) for col in self.serp_columns]
            encodings = [
                self.tokenizer(
                    text,
                    add_special_tokens=True,
                    max_length=self.max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                for text in serp_texts
            ]
            # Stack the tokenized outputs into tensors of shape [num_serp, max_length]
            serp_input_ids = torch.stack([enc['input_ids'].squeeze(0) for enc in encodings])
            serp_attention_mask = torch.stack([enc['attention_mask'].squeeze(0) for enc in encodings])
            self.precomputed_serp_encodings.append({
                'input_ids': serp_input_ids,
                'attention_mask': serp_attention_mask
            })

    def __len__(self):
        return len(self.queries)

    def __getitem__(self, idx):
        query_encoding = self.precomputed_query_encodings[idx]
        serp_encoding = self.precomputed_serp_encodings[idx]
        label = float(self.labels[idx])
        return {
            'query_input_ids': query_encoding['input_ids'].flatten(),
            'query_attention_mask': query_encoding['attention_mask'].flatten(),
            'serp_input_ids': serp_encoding['input_ids'],           # [num_serp, max_length]
            'serp_attention_mask': serp_encoding['attention_mask'],   # [num_serp, max_length]
            'label': torch.tensor(label, dtype=torch.float)
        }

class TeacherBertRegressor(nn.Module):
    def __init__(self,
                 query_bert_model_name='distilbert-base-uncased',
                 serp_bert_model_name='distilbert-base-uncased',
                 dropout=0.1):
        super(TeacherBertRegressor, self).__init__()
        # Separate encoder for queries
        self.query_encoder = DistilBertModel.from_pretrained(query_bert_model_name)
        # Separate encoder for SERP descriptions
        self.result_encoder = DistilBertModel.from_pretrained(serp_bert_model_name)

        # Assume both BERT models produce outputs of the same hidden size
        hidden_size = self.query_encoder.config.hidden_size

        # Fusion layer to combine query and aggregated SERP representations
        self.fusion_layer = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.regressor = nn.Linear(hidden_size, 1)

    def forward(self, query_input_ids, query_attention_mask, serp_input_ids, serp_attention_mask, output_attentions=False):
        # Encode query with optional attention output
        query_outputs = self.query_encoder(
            input_ids=query_input_ids,
            attention_mask=query_attention_mask,
            output_attentions=output_attentions
        )
        #query_rep = query_outputs.pooler_output  # shape: [batch, hidden_size]
        query_rep = query_outputs.last_hidden_state[:, 0]
        # Process SERP descriptions
        batch_size, num_serp, seq_length = serp_input_ids.size()
        serp_input_ids = serp_input_ids.view(batch_size * num_serp, seq_length)
        serp_attention_mask = serp_attention_mask.view(batch_size * num_serp, seq_length)

        serp_outputs = self.result_encoder(
            input_ids=serp_input_ids,
            attention_mask=serp_attention_mask,
            output_attentions=output_attentions
        )
        #serp_rep = serp_outputs.pooler_output  # shape: [batch*num_serp, hidden_size]
        serp_rep = serp_outputs.last_hidden_state[:, 0]

        # Reshape back and aggregate SERP representations (mean pooling)
        serp_rep = serp_rep.view(batch_size, num_serp, -1)
        aggregated_serp = serp_rep.mean(dim=1)  # shape: [batch, hidden_size]

        # Fuse query and SERP representations
        fused = torch.cat([query_rep, aggregated_serp], dim=1)  # shape: [batch, hidden_size*2]
        fused = self.fusion_layer(fused)
        fused = torch.relu(fused)
        fused = self.dropout(fused)

        toxicity_score = self.regressor(fused)

        if output_attentions:
            # Return attention maps from the query encoder for distillation (for example)
            teacher_attns = query_outputs.attentions  # tuple of attention maps
            return toxicity_score.squeeze(), fused, teacher_attns
        else:
            return toxicity_score.squeeze(), fused

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def evaluate_teacher_model(model, data_loader, device):
    model.eval()
    predictions = []
    actual_values = []

    with torch.no_grad():
        for batch in data_loader:
            # Move inputs to device
            query_input_ids = batch['query_input_ids'].to(device)
            query_attention_mask = batch['query_attention_mask'].to(device)
            serp_input_ids = batch['serp_input_ids'].to(device)
            serp_attention_mask = batch['serp_attention_mask'].to(device)
            labels = batch['label'].to(device)

            # Pass output_attentions=False since we don't need them during evaluation
            outputs, _ = model(query_input_ids, query_attention_mask,
                               serp_input_ids, serp_attention_mask,
                               output_attentions=False)

            predictions.extend(outputs.cpu().numpy())
            actual_values.extend(labels.cpu().numpy())

    predictions = np.array(predictions)
    actual_values = np.array(actual_values)

    return predictions

TEXT_COLUMN = 'text'  # Change these column names to match your CSV
LABEL_COLUMN = 'score'
MAX_LENGTH = 24

results = {}
for fold, df_test in fold_data.items():
    # a) build DataLoader
    ds     = PrecomputedTeacherTextRegressionDataset(
                 df_test,
                 tokenizer,
                 TEXT_COLUMN,
                 LABEL_COLUMN,
                 MAX_LENGTH
             )
    loader = DataLoader(ds, batch_size=41, shuffle=False)

    # b) load model weights
    # b) load model weights
    #model_file = hf_hub_download(
    #    repo_id=f"ppaudel/loki-model-teacher-fold-{fold[-1]}",
    #    filename="pytorch_model.bin",
    #    repo_type="model"     ,
    #    cache_dir="./HF_cache/"
    #)
    model_file = os.path.join(ZENODO_MODEL_DIR, f"loki-model-teacher-fold-{fold[-1]}", "pytorch_model.bin")
    teacher_model = TeacherBertRegressor().to(device)
    state_dict    = torch.load(model_file, map_location=device,weights_only=True)
    teacher_model.load_state_dict(state_dict, strict=True)
    teacher_model.eval()

    # c) run evaluation
    preds= evaluate_teacher_model(teacher_model, loader, device)

    # d) read CSV & attach predictions
    #df = pd.read_csv(csv_path)
    df = df_test.to_pandas()
    df['predicted_label'] = preds

    # e) pick top‑20 by prediction
    top20 = df.nlargest(20, 'predicted_label')

    # f) compute means
    top_tox = top20[LABEL_COLUMN].mean()
    top_exp = np.mean([keyword_score_expansion[t] for t in top20['text']])

    # g) store
    results[fold] = {
        'Tox.': top_tox,
        'Exp.': top_exp
    }

folds   = list(results.keys())
metrics = ['Tox.','Exp.']
cols    = pd.MultiIndex.from_product([folds, metrics],
                                     names=['Fold','Metric'])

perf = pd.DataFrame(index=['LOKI Teacher Model'], columns=cols)

for fold in folds:
    perf.loc['LOKI Teacher Model', (fold, 'Tox.')] = results[fold]['Tox.']
    perf.loc['LOKI Teacher Model', (fold, 'Exp.')] = results[fold]['Exp.']

print(perf)

"""### Table 3 : Student  Model"""

from transformers import DistilBertModel, DistilBertTokenizer

class StudentBertRegressor(nn.Module):
    def __init__(self, bert_model_name='distilbert-base-uncased', dropout=0.1):
        super(StudentBertRegressor, self).__init__()
        self.bert = DistilBertModel.from_pretrained(bert_model_name)
        hidden_size = self.bert.config.hidden_size

        # Head for toxicity prediction
        self.prediction_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

        # Head for distillation (producing a "hint" representation)
        self.distill_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )

    def forward(self, query_input_ids, query_attention_mask, output_attentions=False):
        outputs = self.bert(
            input_ids=query_input_ids,
            attention_mask=query_attention_mask,
            output_attentions=output_attentions
        )
        # Use the first token representation as the sentence embedding
        query_rep = outputs.last_hidden_state[:, 0]  # shape: [batch, hidden_size]
        pred = self.prediction_head(query_rep)       # toxicity prediction
        hint = self.distill_head(query_rep)            # intermediate representation for distillation
        if output_attentions:
            # Return attentions along with prediction and hint.
            # outputs.attentions is a tuple containing attention maps from all layers.
            return pred.squeeze(), hint, outputs.attentions
        else:
            return pred.squeeze(), hint

def evaluate_student_model(student_model, data_loader, device):
    """
    Evaluate the student model (query-only) and return predictions and regression metrics.

    Assumes each batch in data_loader contains:
      - 'query_input_ids'
      - 'query_attention_mask'
      - 'label'

    Returns:
      - predictions: np.array of predicted toxicity scores
      - actual_values: np.array of ground truth labels
      - metrics: a dictionary containing mse, rmse, mae, and r2 scores.
    """
    student_model.eval()
    predictions = []
    actual_values = []

    with torch.no_grad():
        for batch in data_loader:
            query_input_ids = batch['query_input_ids'].to(device)
            query_attention_mask = batch['query_attention_mask'].to(device)
            labels = batch['label'].to(device)

            # Use mixed precision for faster inference
            #with torch.cuda.amp.autocast():
                # Student model returns (toxicity prediction, hint), we only need the prediction.
            outputs, _ = student_model(query_input_ids, query_attention_mask)

            predictions.extend(outputs.cpu().numpy())
            actual_values.extend(labels.cpu().numpy())

    predictions = np.array(predictions)

    return predictions

results = {}
for fold, df_test in fold_data.items():
    # a) build DataLoader
    ds     = PrecomputedTeacherTextRegressionDataset(
                 df_test,
                 tokenizer,
                 TEXT_COLUMN,
                 LABEL_COLUMN,
                 MAX_LENGTH
             )
    loader = DataLoader(ds, batch_size=41, shuffle=False)

    # b) load model weights
    #model_file = hf_hub_download(
    #    repo_id=f"ppaudel/loki-model-student-fold-{fold[-1]}",
    #    filename="pytorch_model.bin",
    #    repo_type="model"                  # since you uploaded via repo_type="model"
    #)
    model_file = os.path.join(ZENODO_MODEL_DIR, f"loki-model-student-fold-{fold[-1]}", "pytorch_model.bin")
    student_model = StudentBertRegressor().to(device)
    state_dict    = torch.load(model_file, map_location=device,weights_only=True)
    student_model.load_state_dict(state_dict, strict=True)
    student_model.eval()

    # c) run evaluation
    preds = evaluate_student_model(student_model, loader, device)

    # d) read CSV & attach predictions
    #df = pd.read_csv(csv_path)
    df = df_test.to_pandas()
    df['predicted_label'] = preds

    # e) pick top‑20 by prediction
    top20 = df.sort_values(by='predicted_label', ascending=False).head(20)


    # f) compute means
    top_tox = top20[LABEL_COLUMN].mean()
    top_exp = np.mean([keyword_score_expansion[t] for t in top20['text']])

    # g) store
    results[fold] = {
        'Tox.': top_tox,
        'Exp.': top_exp
    }

folds   = list(results.keys())
metrics = ['Tox.','Exp.']
cols    = pd.MultiIndex.from_product([folds, metrics],
                                     names=['Fold','Metric'])

perf = pd.DataFrame(index=['LOKI Student Model'], columns=cols)

for fold in folds:
    perf.loc['LOKI Student Model', (fold, 'Tox.')] = results[fold]['Tox.']
    perf.loc['LOKI Student Model', (fold, 'Exp.')] = results[fold]['Exp.']

print(perf)
